{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EISHKARAN/TSS-Resources/blob/main/Tokenization_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAFRy8tgl-36"
      },
      "source": [
        "##### About\n",
        "Tokenization is one of the first steps of data processing when it comes to working with data in the domain of NLP.\n",
        "\n",
        "* We will use spacy to tokenize input sentences and compare it's results with basic tokenization performed via Python.\n",
        "\n",
        "\n",
        "#### Requirements\n",
        "```\n",
        "pip install spacy\n",
        "python -m spacy download en_core_web_sm\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIRh9Fq3l-3-",
        "outputId": "093941a8-95b3-4018-da43-c3c3403d8806"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token 0 - Hi,\n",
            "Token 1 - There\n",
            "Token 2 - !\n",
            "Token 3 - This\n",
            "Token 4 - is\n",
            "Token 5 - a\n",
            "Token 6 - notebook\n",
            "Token 7 - on\n",
            "Token 8 - Tokenization\n"
          ]
        }
      ],
      "source": [
        "# tokenization of a text using python\n",
        "doc = \"Hi, There ! This is a notebook on Tokenization\"\n",
        "for i,token in enumerate(doc.split(\" \")):\n",
        "    print(\"Token {} - {}\".format(i,token))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeXJUd7El-4B"
      },
      "source": [
        "But, This straightforward approach of tokenisation encounters a lot of loopholes as text contains tokens which are noisy. Like associated with hyphens or name of various nouns.\n",
        "\n",
        "* BERT uses the concept of sub-word tokens to permute over various combinations of characters which can form part of the vocabulary. It helps it in narrowing down to the OOV(Out of vocabulary) tokens.\n",
        "\n",
        "Thus, We use spacy as an efficient tokenizer for NLP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhHQ59dnl-4C",
        "outputId": "f595bef6-3fb9-4169-997c-7ba8f93c935f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token: Hi\n",
            "Token: ,\n",
            "Token: There\n",
            "Token: !\n",
            "Token: This\n",
            "Token: is\n",
            "Token: a\n",
            "Token: notebook\n",
            "Token: on\n",
            "Token: Tokenization\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "#tokenizigng\n",
        "doc = nlp(\"Hi, There ! This is a notebook on Tokenization\")\n",
        "for token in doc:\n",
        "    print(\"Token: {}\".format(token))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95I1iS6kl-4E"
      },
      "source": [
        "One can also add his own tokenizer rules. Visit  <a href=\"https://spacy.io/usage/linguistic-features#special-cases\"> Link </a>\n",
        "\n",
        "Besides this, Each model like BERT, BART and its variants come with their own tokenizers. Let's have a look at one such variant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFeNh6jhl-4E"
      },
      "outputs": [],
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "sentences = [\n",
        "    \"Hi, This is our first Tokenizer Notebook\",\n",
        "    \"Glad to see you here.\",\n",
        "    \"What are you upto ?\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYgHnB6Bl-4F",
        "outputId": "4424946b-f9a5-423d-a08f-0b79ad65a797"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'you': 1, 'hi': 2, 'this': 3, 'is': 4, 'our': 5, 'first': 6, 'tokenizer': 7, 'notebook': 8, 'glad': 9, 'to': 10, 'see': 11, 'here': 12, 'what': 13, 'are': 14, 'upto': 15}\n"
          ]
        }
      ],
      "source": [
        "tokenizer = Tokenizer(num_words=20)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_idx = tokenizer.word_index\n",
        "print(word_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kag4NFHml-4H",
        "outputId": "1f5dbbb7-862d-4c02-e914-2300669d2d4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 3, 4, 5, 6, 7, 8]\n",
            "[9, 10, 11, 1, 12]\n",
            "[13, 14, 1, 15]\n"
          ]
        }
      ],
      "source": [
        "# converting each tokenized sentence into sequence\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "for seq in sequences:\n",
        "    print(seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmQ--CPcl-4I",
        "outputId": "bff4d515-a25b-412f-8355-95a32d7fe23d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2 3 4 5 6 7 8]\n",
            "[ 9 10 11  1 12  0  0]\n",
            "[13 14  1 15  0  0  0]\n"
          ]
        }
      ],
      "source": [
        "# to ensure that each sequence contains same number of tokens which are a primary need for any NN. We'll pad\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "padded_sequences = pad_sequences(sequences, padding='post')\n",
        "for seq in padded_sequences:\n",
        "    print(seq)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11 (default, Jul 27 2021, 14:32:16) \n[GCC 7.5.0]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "50052c996937e9a0e161d422489677fdaadc23d756ac209b7397e80e5ea8cea0"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}